{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library and Generate train-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_relukan import ReLUKANLayer, ReLUKAN\n",
    "from torch_hrkan import HRKANLayer, HRKAN\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.integrate._ivp.radau import P\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "from kan import KAN, LBFGS\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from fft_burgers import fft_burgers\n",
    "\n",
    "nx = 199\n",
    "nt = 199\n",
    "nu=0.001\n",
    "xmin=0\n",
    "xmax=2.5\n",
    "tmin=0\n",
    "tmax=2.5\n",
    "\n",
    "hx = (xmax - xmin) / nx\n",
    "ht = (tmax - tmin) / nt\n",
    "noise_xstd = hx / 4.0\n",
    "noise_tstd = ht / 4.0\n",
    "\n",
    "alpha = 0.05\n",
    "dim = 2\n",
    "np_i = 100 \n",
    "np_b = 100 \n",
    "\n",
    "def batch_jacobian(func, x, create_graph=False):\n",
    "    def _func_sum(x):\n",
    "        return func(x).sum(dim=0)\n",
    "    return autograd.functional.jacobian(_func_sum, x, create_graph=create_graph).permute(1,0,2)\n",
    "\n",
    "# interior\n",
    "sampling_mode = 'random'\n",
    "\n",
    "x_mesh = torch.linspace(xmin, xmax, np_i)\n",
    "y_mesh = torch.linspace(tmin, tmax, np_i)\n",
    "X, Y = torch.meshgrid(x_mesh, y_mesh, indexing=\"ij\")\n",
    "if sampling_mode == 'mesh':\n",
    "    #mesh\n",
    "    x_i = torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1,0)\n",
    "else:\n",
    "    #random\n",
    "    x_i = torch.hstack([torch.rand((np_i**2,1))*xmax, torch.rand((np_i**2,1))*tmax])\n",
    "\n",
    "# boundary, 4 sides\n",
    "helper = lambda X, Y: torch.stack([X.reshape(-1,), Y.reshape(-1,)]).permute(1,0)\n",
    "xb1 = helper(X[0], Y[0])\n",
    "xb2 = helper(X[-1], Y[0])\n",
    "xb3 = helper(X[:,0], Y[:,0])\n",
    "xb4 = helper(X[:,0], Y[:,-1])\n",
    "x_bx = torch.cat([xb1, xb2], dim=0)\n",
    "x_bt = xb3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_mode_test = 'mesh'\n",
    "\n",
    "x_test_mesh = torch.linspace(xmin, xmax, (nx*8)+1)[::8]\n",
    "y_test_mesh = torch.linspace(tmin, tmax, nt+1)\n",
    "X_test, Y_test = torch.meshgrid(x_test_mesh, y_test_mesh, indexing=\"ij\")\n",
    "if sampling_mode_test == 'mesh':\n",
    "    #mesh\n",
    "    x_test_i = torch.stack([X_test.reshape(-1,), Y_test.reshape(-1,)]).permute(1,0)\n",
    "else:\n",
    "    #random\n",
    "    x_test_i = torch.hstack([torch.rand((nx**2,1))*xmax, torch.rand((nt**2,1))*tmax])\n",
    "    \n",
    "xb1_test = helper(X_test[0], Y_test[0])\n",
    "xb2_test = helper(X_test[-1], Y_test[0])\n",
    "xb3_test = helper(X_test[:,0], Y_test[:,0])\n",
    "xb4_test = helper(X_test[:,0], Y_test[:,-1])\n",
    "x_test_b = torch.cat([xb1_test, xb2_test, xb3_test, xb4_test], dim=0)\n",
    "# x_test_bt = xb3_test\n",
    "x_test_bx = torch.cat([xb1_test, xb2_test], dim=0)\n",
    "x_test_bt = xb3_test\n",
    "\n",
    "X_test_np = X_test.clone().detach().numpy()\n",
    "Y_test_np = Y_test.clone().detach().numpy()\n",
    "x_test = torch.stack([X_test.reshape(-1,), Y_test.reshape(-1,)]).permute(1,0)\n",
    "\n",
    "def get_sol(x, t):\n",
    "    \"\"\" use FFT method \"\"\"\n",
    "    x = np.linspace(xmin, xmax, (nx*8)+1)\n",
    "    t = np.linspace(tmin, tmax, nt+1)\n",
    "    sol = fft_burgers(x, t, nu)\n",
    "    sol = sol[:,::8]\n",
    "    sol = sol.T\n",
    "    return sol\n",
    "\n",
    "sol = get_sol(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check if have GPU and move data there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x_i = x_i.cuda()\n",
    "    x_bx = x_bx.cuda()\n",
    "    x_bt = x_bt.cuda()\n",
    "    x_test = x_test.cuda()\n",
    "    x_test_i = x_test_i.cuda()\n",
    "    x_test_bx = x_test_bx.cuda()\n",
    "    x_test_bt = x_test_bt.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the function to train ReLUKAN and HRKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    plt.ion()\n",
    "    losses = []\n",
    "    pde_losses = []\n",
    "    bc_losses = []\n",
    "    pde_losses_test = []\n",
    "    bc_losses_test = []\n",
    "    l2_losses_test = []\n",
    "    l2_losses_std_test= []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for e in range(3000):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        sol_D1_fun = lambda x: batch_jacobian(model, x, create_graph=True)[:,0,:]\n",
    "        sol_D1_x = sol_D1_fun(x_i)[:,[0]]\n",
    "        sol_D1_t = sol_D1_fun(x_i)[:,[1]]\n",
    "        \n",
    "        sol_D1_fun_x = lambda x: sol_D1_fun(x)[:,[0]]\n",
    "        sol_D2 = batch_jacobian(sol_D1_fun_x, x_i, create_graph=True)[:,:,0]\n",
    "        \n",
    "        pde_loss = torch.mean((sol_D1_t + model(x_i) * sol_D1_x/4 - nu * sol_D2/16)**2)\n",
    "        bc_loss = torch.mean((model(x_bx))**2) + torch.mean((model(x_bt)-1/torch.cosh(4*x_bt[:,[0]]-5))**2)\n",
    "        loss = alpha * pde_loss + bc_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            sol_D1_x_test = sol_D1_fun(x_test_i)[:,[0]]\n",
    "            sol_D1_t_test = sol_D1_fun(x_test_i)[:,[1]]\n",
    "            \n",
    "            sol_D2_test = batch_jacobian(sol_D1_fun_x, x_test_i, create_graph=False)[:,:,0]\n",
    "            \n",
    "            pde_loss_test = torch.mean((sol_D1_t_test + model(x_test_i) * sol_D1_x_test/4 - nu * sol_D2_test/16)**2)\n",
    "            bc_loss_test = torch.mean((model(x_test_bx))**2) + torch.mean((model(x_test_bt)-1/torch.cosh(4*x_test_bt[:,[0]]-5))**2)\n",
    "\n",
    "            l2_test = torch.mean((model(x_test).cpu().clone().detach() - sol.reshape(-1,1))**2)\n",
    "            l2_test_std = torch.std((model(x_test).cpu().clone().detach() - sol.reshape(-1,1))**2)\n",
    "    \n",
    "            pde_losses.append(pde_loss.cpu().detach().numpy())\n",
    "            bc_losses.append(bc_loss.cpu().detach().numpy())\n",
    "            pde_losses_test.append(pde_loss_test.cpu().detach().numpy())\n",
    "            bc_losses_test.append(bc_loss_test.cpu().detach().numpy())\n",
    "            l2_losses_test.append(l2_test.cpu().detach().numpy())\n",
    "            l2_losses_std_test.append(l2_test_std.cpu().detach().numpy())\n",
    "\n",
    "    elapsed = (time.time() - start)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(x_test).cpu().clone().detach().numpy().reshape(X_test_np.shape)\n",
    "    \n",
    "    return output, losses, pde_losses, bc_losses, pde_losses_test, bc_losses_test, l2_losses_test, l2_losses_std_test, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the function to train KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kan(model):\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    plt.ion()\n",
    "    losses = []\n",
    "    pde_losses = []\n",
    "    bc_losses = []\n",
    "    pde_losses_test = []\n",
    "    bc_losses_test = []\n",
    "    l2_losses_test = []\n",
    "    l2_losses_std_test= []\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for e in range(3000):\n",
    "        \n",
    "        if e % 5 == 0 and e < 50:\n",
    "            model.update_grid_from_samples(x_i)\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        sol_D1_fun = lambda x: batch_jacobian(model, x, create_graph=True)[:,0,:]\n",
    "        sol_D1_x = sol_D1_fun(x_i)[:,[0]]\n",
    "        sol_D1_t = sol_D1_fun(x_i)[:,[1]]\n",
    "        \n",
    "        sol_D1_fun_x = lambda x: sol_D1_fun(x)[:,[0]]\n",
    "        sol_D2 = batch_jacobian(sol_D1_fun, x_i, create_graph=True)[:,:,0]\n",
    "        \n",
    "        pde_loss = torch.mean((sol_D1_t + model(x_i) * sol_D1_x/4 - nu * sol_D2/16)**2)\n",
    "        bc_loss = torch.mean((model(x_bx))**2) + torch.mean((model(x_bt)-1/torch.cosh(4*x_bt[:,[0]]-5))**2)\n",
    "        loss = alpha * pde_loss + bc_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            sol_D1_x_test = sol_D1_fun(x_test_i)[:,[0]]\n",
    "            sol_D1_t_test = sol_D1_fun(x_test_i)[:,[1]]\n",
    "            \n",
    "            sol_D2_test = batch_jacobian(sol_D1_fun, x_test_i, create_graph=False)[:,:,0]\n",
    "            \n",
    "            pde_loss_test = torch.mean((sol_D1_t_test + model(x_test_i) * sol_D1_x_test/4 - nu * sol_D2_test/16)**2)\n",
    "            bc_loss_test = torch.mean((model(x_test_bx))**2) + torch.mean((model(x_test_bt)-1/torch.cosh(4*x_test_bt[:,[0]]-5))**2)\n",
    "\n",
    "            l2_test = torch.mean((model(x_test).cpu().clone().detach() - sol.reshape(-1,1))**2)\n",
    "            l2_test_std = torch.std((model(x_test).cpu().clone().detach() - sol.reshape(-1,1))**2)\n",
    "    \n",
    "            pde_losses.append(pde_loss.cpu().detach().numpy())\n",
    "            bc_losses.append(bc_loss.cpu().detach().numpy())\n",
    "            pde_losses_test.append(pde_loss_test.cpu().detach().numpy())\n",
    "            bc_losses_test.append(bc_loss_test.cpu().detach().numpy())\n",
    "            l2_losses_test.append(l2_test.cpu().detach().numpy())\n",
    "            l2_losses_std_test.append(l2_test_std.cpu().detach().numpy())\n",
    "\n",
    "    elapsed = (time.time() - start)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(x_test).cpu().clone().detach().numpy().reshape(X_test_np.shape)\n",
    "    \n",
    "    return output, losses, pde_losses, bc_losses, pde_losses_test, bc_losses_test, l2_losses_test, l2_losses_std_test, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the function to plot ground-truth solution, learnt solutions and their residual difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig(relu_kan_, hrkan_, kan_, i):\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(25,8), subplot_kw={\"projection\": \"3d\"})\n",
    "    fig.suptitle('Solutions and their residual difference')\n",
    "    \n",
    "    #true solution\n",
    "    surf1 = axs[0,0].plot_surface(X_test_np, Y_test_np, sol.reshape(X_test_np.shape), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[0,0].set_zlim(-1.01, 1.01)\n",
    "    axs[0,0].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[0,0].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[0,0].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"Ground-truth\")\n",
    "    \n",
    "    # RELU_KAN\n",
    "    surf2 = axs[0,1].plot_surface(X_test_np, Y_test_np, relu_kan_[0], cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[0,1].set_zlim(-1.01, 1.01)\n",
    "    axs[0,1].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[0,1].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[0,1].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"ReLU-Kan solution\")\n",
    "    \n",
    "    surf3 = axs[1,1].plot_surface(X_test_np, Y_test_np, (sol - relu_kan_[0]), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[1,1].set_zlim(-1.01, 1.01)\n",
    "    axs[1,1].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[1,1].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[1,1].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"ReLU-Kan residual\")\n",
    "    \n",
    "    # HRKAN\n",
    "    surf4 = axs[0,2].plot_surface(X_test_np, Y_test_np, hrkan_[0], cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[0,2].set_zlim(-1.01, 1.01)\n",
    "    axs[0,2].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[0,2].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[0,2].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"HRKan solution\")\n",
    "    \n",
    "    surf5 = axs[1,2].plot_surface(X_test_np, Y_test_np, (sol - hrkan_[0]), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[1,2].set_zlim(-1.01, 1.01)\n",
    "    axs[1,2].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[1,2].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[1,2].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"HRKan residual\")\n",
    "    \n",
    "    # KAN\n",
    "    surf6 = axs[0,3].plot_surface(X_test_np, Y_test_np, kan_[0], cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[0,3].set_zlim(-1.01, 1.01)\n",
    "    axs[0,3].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[0,3].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[0,3].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"Kan solution\")\n",
    "    \n",
    "    surf7 = axs[1,3].plot_surface(X_test_np, Y_test_np, (sol - kan_[0]), cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "    axs[1,3].set_zlim(-1.01, 1.01)\n",
    "    axs[1,3].zaxis.set_major_locator(LinearLocator(10))\n",
    "    axs[1,3].zaxis.set_major_formatter('{x:.02f}')\n",
    "    axs[1,3].set(xlabel='x', ylabel='t', zlabel='u(x,t)', title=\"Kan residual\")\n",
    "    \n",
    "    cb1 = fig.colorbar(surf1, ax=axs, orientation='vertical')\n",
    "\n",
    "    plt.savefig(f'Burgersv_fig_{i}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define the function to calculate MSE, MSE std. and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_kan_loss, relu_kan_loss_test, relu_kan_L2s, relu_kan_L2s_std, relu_kan_time = [], [], [], [], []\n",
    "hrkan_loss, hrkan_loss_test, hrkan_L2s, hrkan_L2s_std, hrkan_time = [], [], [], [], []\n",
    "kan_loss, kan_loss_test, kan_L2s, kan_L2s_std, kan_time = [], [], [], [], []\n",
    "def cal_error(relu_kan_, hrkan_, kan_, i):\n",
    "    relu_kan_loss.append([alpha * x + y for x, y in zip(relu_kan_[2], relu_kan_[3])])\n",
    "    relu_kan_loss_test.append([alpha * x + y for x, y in zip(relu_kan_[4], relu_kan_[5])])\n",
    "    relu_kan_L2s.append(relu_kan_[-3])\n",
    "    relu_kan_L2s_std.append(relu_kan_[-2])\n",
    "    relu_kan_time.append(relu_kan_[-1])\n",
    "    hrkan_loss.append([alpha * x + y for x, y in zip(hrkan_[2], hrkan_[3])])\n",
    "    hrkan_loss_test.append([alpha * x + y for x, y in zip(hrkan_[4], hrkan_[5])])\n",
    "    hrkan_L2s.append(hrkan_[-3])\n",
    "    hrkan_L2s_std.append(hrkan_[-2])\n",
    "    hrkan_time.append(hrkan_[-1])\n",
    "    kan_loss.append([alpha * x + y for x, y in zip(kan_[2], kan_[3])])\n",
    "    kan_loss_test.append([alpha * x + y for x, y in zip(kan_[4], kan_[5])])\n",
    "    kan_L2s.append(kan_[-3])\n",
    "    kan_L2s_std.append(kan_[-2])\n",
    "    kan_time.append(kan_[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_kans = []\n",
    "hrkans = []\n",
    "kans = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    relu_kan = ReLUKAN([2,3,3,3,1], 7, 3, 0, 2.5)\n",
    "    relu_kan = relu_kan.cuda()\n",
    "    relu_kan_results = train_model(relu_kan)\n",
    "    del relu_kan\n",
    "    hrkan = HRKAN([2,3,3,3,1], 7, 3, 0, 2.5, 4)\n",
    "    hrkan = hrkan.cuda()\n",
    "    hrkan_results = train_model(hrkan)\n",
    "    del hrkan\n",
    "    kan = KAN(width=[2,3,3,3,1], grid=7, k=3, grid_eps=1.0, device='cuda')\n",
    "    kan_results = train_kan(kan)\n",
    "    del hrkan\n",
    "    plot_fig(relu_kan_results, hrkan_results, kan_results, i)\n",
    "    cal_error(relu_kan_results, hrkan_results, kan_results, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot the median losses and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_median=5\n",
    "q1_quantile = 0\n",
    "q2_quantile = -1\n",
    "fig, axs = plt.subplots(3, 3, figsize=(25,18))\n",
    "fig.suptitle('Loss and accuracy (median and max-min-band of 10 runs)')\n",
    "\n",
    "axs[0,0].plot(np.arange(3000),np.sort(np.array(hrkan_loss), axis=0)[k_median,:])\n",
    "axs[0,0].plot(np.arange(3000),np.sort(np.array(relu_kan_loss), axis=0)[k_median,:])\n",
    "axs[0,0].plot(np.arange(3000),np.sort(np.array(kan_loss), axis=0)[k_median,:])\n",
    "axs[0,0].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[0,0].fill_between(np.arange(3000),np.sort(np.array(hrkan_loss), axis=0)[q1_quantile,:], np.sort(np.array(hrkan_loss), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[0,0].fill_between(np.arange(3000),np.sort(np.array(relu_kan_loss), axis=0)[q1_quantile,:], np.sort(np.array(relu_kan_loss), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[0,0].fill_between(np.arange(3000),np.sort(np.array(kan_loss), axis=0)[q1_quantile,:], np.sort(np.array(kan_loss), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[0,0].set(xlabel='epoch', ylabel='loss', title=\"Training loss\")\n",
    "\n",
    "axs[0,1].plot(np.arange(1000,3000),np.sort(np.array(hrkan_loss), axis=0)[k_median,1000:])\n",
    "axs[0,1].plot(np.arange(1000,3000),np.sort(np.array(relu_kan_loss), axis=0)[k_median,1000:])\n",
    "axs[0,1].plot(np.arange(1000,3000),np.sort(np.array(kan_loss), axis=0)[k_median,1000:])\n",
    "axs[0,1].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[0,1].fill_between(np.arange(1000,3000),np.sort(np.array(hrkan_loss), axis=0)[q1_quantile,1000:], np.sort(np.array(hrkan_loss), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[0,1].fill_between(np.arange(1000,3000),np.sort(np.array(relu_kan_loss), axis=0)[q1_quantile,1000:], np.sort(np.array(relu_kan_loss), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[0,1].fill_between(np.arange(1000,3000),np.sort(np.array(kan_loss), axis=0)[q1_quantile,1000:], np.sort(np.array(kan_loss), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[0,1].set(xlabel='epoch', ylabel='loss', title=\"Training loss\")\n",
    "\n",
    "axs[0,2].plot(np.arange(2000,3000),np.sort(np.array(hrkan_loss), axis=0)[k_median,2000:])\n",
    "axs[0,2].plot(np.arange(2000,3000),np.sort(np.array(relu_kan_loss), axis=0)[k_median,2000:])\n",
    "axs[0,2].plot(np.arange(2000,3000),np.sort(np.array(kan_loss), axis=0)[k_median,2000:])\n",
    "axs[0,2].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[0,2].fill_between(np.arange(2000,3000),np.sort(np.array(hrkan_loss), axis=0)[q1_quantile,2000:], np.sort(np.array(hrkan_loss), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[0,2].fill_between(np.arange(2000,3000),np.sort(np.array(relu_kan_loss), axis=0)[q1_quantile,2000:], np.sort(np.array(relu_kan_loss), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[0,2].fill_between(np.arange(2000,3000),np.sort(np.array(kan_loss), axis=0)[q1_quantile,2000:], np.sort(np.array(kan_loss), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[0,2].set(xlabel='epoch', ylabel='loss', title=\"Training loss\")\n",
    "\n",
    "axs[1,0].plot(np.arange(3000),np.sort(np.array(hrkan_loss_test), axis=0)[k_median,:])\n",
    "axs[1,0].plot(np.arange(3000),np.sort(np.array(relu_kan_loss_test), axis=0)[k_median,:])\n",
    "axs[1,0].plot(np.arange(3000),np.sort(np.array(kan_loss_test), axis=0)[k_median,:])\n",
    "axs[1,0].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[1,0].fill_between(np.arange(3000),np.sort(np.array(hrkan_loss_test), axis=0)[q1_quantile,:], np.sort(np.array(hrkan_loss_test), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[1,0].fill_between(np.arange(3000),np.sort(np.array(relu_kan_loss_test), axis=0)[q1_quantile,:], np.sort(np.array(relu_kan_loss_test), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[1,0].fill_between(np.arange(3000),np.sort(np.array(kan_loss_test), axis=0)[q1_quantile,:], np.sort(np.array(kan_loss_test), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[1,0].set(xlabel='epoch', ylabel='loss', title=\"Test loss\")\n",
    "\n",
    "axs[1,1].plot(np.arange(1000,3000),np.sort(np.array(hrkan_loss_test), axis=0)[k_median,1000:])\n",
    "axs[1,1].plot(np.arange(1000,3000),np.sort(np.array(relu_kan_loss_test), axis=0)[k_median,1000:])\n",
    "axs[1,1].plot(np.arange(1000,3000),np.sort(np.array(kan_loss_test), axis=0)[k_median,1000:])\n",
    "axs[1,1].fill_between(np.arange(1000,3000),np.sort(np.array(hrkan_loss_test), axis=0)[q1_quantile,1000:], np.sort(np.array(hrkan_loss_test), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[1,1].fill_between(np.arange(1000,3000),np.sort(np.array(relu_kan_loss_test), axis=0)[q1_quantile,1000:], np.sort(np.array(relu_kan_loss_test), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[1,1].fill_between(np.arange(1000,3000),np.sort(np.array(kan_loss_test), axis=0)[q1_quantile,1000:], np.sort(np.array(kan_loss_test), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[1,1].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[1,1].set(xlabel='epoch', ylabel='loss', title=\"Test loss\")\n",
    "\n",
    "axs[1,2].plot(np.arange(2000,3000),np.sort(np.array(hrkan_loss_test), axis=0)[k_median,2000:])\n",
    "axs[1,2].plot(np.arange(2000,3000),np.sort(np.array(relu_kan_loss_test), axis=0)[k_median,2000:])\n",
    "axs[1,2].plot(np.arange(2000,3000),np.sort(np.array(kan_loss_test), axis=0)[k_median,2000:])\n",
    "axs[1,2].fill_between(np.arange(2000,3000),np.sort(np.array(hrkan_loss_test), axis=0)[q1_quantile,2000:], np.sort(np.array(hrkan_loss_test), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[1,2].fill_between(np.arange(2000,3000),np.sort(np.array(relu_kan_loss_test), axis=0)[q1_quantile,2000:], np.sort(np.array(relu_kan_loss_test), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[1,2].fill_between(np.arange(2000,3000),np.sort(np.array(kan_loss_test), axis=0)[q1_quantile,2000:], np.sort(np.array(kan_loss_test), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[1,2].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[1,2].set(xlabel='epoch', ylabel='loss', title=\"Test loss\")\n",
    "\n",
    "axs[2,0].plot(np.arange(3000),np.sort(np.array(hrkan_L2s), axis=0)[k_median,:])\n",
    "axs[2,0].plot(np.arange(3000),np.sort(np.array(relu_kan_L2s), axis=0)[k_median,:])\n",
    "axs[2,0].plot(np.arange(3000),np.sort(np.array(kan_L2s), axis=0)[k_median,:])\n",
    "axs[2,0].fill_between(np.arange(3000),np.sort(np.array(hrkan_L2s), axis=0)[q1_quantile,:], np.sort(np.array(hrkan_L2s), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[2,0].fill_between(np.arange(3000),np.sort(np.array(relu_kan_L2s), axis=0)[q1_quantile,:], np.sort(np.array(relu_kan_L2s), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[2,0].fill_between(np.arange(3000),np.sort(np.array(kan_L2s), axis=0)[q1_quantile,:], np.sort(np.array(kan_L2s), axis=0)[q2_quantile,:], alpha=0.3)\n",
    "axs[2,0].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[2,0].set(xlabel='epoch', ylabel='MSE', title=\"Test MSE\")\n",
    "\n",
    "axs[2,1].plot(np.arange(1000,3000),np.sort(np.array(hrkan_L2s), axis=0)[k_median,1000:])\n",
    "axs[2,1].plot(np.arange(1000,3000),np.sort(np.array(relu_kan_L2s), axis=0)[k_median,1000:])\n",
    "axs[2,1].plot(np.arange(1000,3000),np.sort(np.array(kan_L2s), axis=0)[k_median,1000:])\n",
    "axs[2,1].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[2,1].fill_between(np.arange(1000,3000),np.sort(np.array(hrkan_L2s), axis=0)[q1_quantile,1000:], np.sort(np.array(hrkan_L2s), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[2,1].fill_between(np.arange(1000,3000),np.sort(np.array(relu_kan_L2s), axis=0)[q1_quantile,1000:], np.sort(np.array(relu_kan_L2s), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[2,1].fill_between(np.arange(1000,3000),np.sort(np.array(kan_L2s), axis=0)[q1_quantile,1000:], np.sort(np.array(kan_L2s), axis=0)[q2_quantile,1000:], alpha=0.3)\n",
    "axs[2,1].set(xlabel='epoch', ylabel='MSE', title=\"Test MSE\")\n",
    "\n",
    "axs[2,2].plot(np.arange(2000,3000),np.sort(np.array(hrkan_L2s), axis=0)[k_median,2000:])\n",
    "axs[2,2].plot(np.arange(2000,3000),np.sort(np.array(relu_kan_L2s), axis=0)[k_median,2000:])\n",
    "axs[2,2].plot(np.arange(2000,3000),np.sort(np.array(kan_L2s), axis=0)[k_median,2000:])\n",
    "axs[2,2].legend(['HRKan', 'ReLU-Kan', 'Kan'])\n",
    "axs[2,2].fill_between(np.arange(2000,3000),np.sort(np.array(hrkan_L2s), axis=0)[q1_quantile,2000:], np.sort(np.array(hrkan_L2s), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[2,2].fill_between(np.arange(2000,3000),np.sort(np.array(relu_kan_L2s), axis=0)[q1_quantile,2000:], np.sort(np.array(relu_kan_L2s), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[2,2].fill_between(np.arange(2000,3000),np.sort(np.array(kan_L2s), axis=0)[q1_quantile,2000:], np.sort(np.array(kan_L2s), axis=0)[q2_quantile,2000:], alpha=0.3)\n",
    "axs[2,2].set(xlabel='epoch', ylabel='MSE', title=\"Test MSE\")\n",
    "\n",
    "plt.savefig(f'Burgersv_loss_band.png', dpi=400)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Calculate mean MSE, mean MSE std. and mean training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"{0:.5g}\".format(np.array(relu_kan_L2s)[:,-1].mean()), \"{0:.5g}\".format(np.array(relu_kan_L2s_std)[:,-1].mean()), \"{0:.5g}\".format(np.array(relu_kan_time)[-1].mean()))\n",
    "print( \"{0:.5g}\".format(np.array(hrkan_L2s)[:,-1].mean()), \"{0:.5g}\".format(np.array(hrkan_L2s_std)[:,-1].mean()), \"{0:.5g}\".format(np.array(hrkan_time)[-1].mean()))\n",
    "print( \"{0:.5g}\".format(np.array(kan_L2s)[:,-1].mean()), \"{0:.5g}\".format(np.array(kan_L2s_std)[:,-1].mean()), \"{0:.5g}\".format(np.array(kan_time)[-1].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ANN",
   "language": "python",
   "name": "ann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
